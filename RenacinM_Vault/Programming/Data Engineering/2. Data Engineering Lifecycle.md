*Notes Taken From: Fundamentals Of Data Engineering* - Joe Reis & Matt Housley

The data engineering lifecycle comprises stages that turn raw data ingredients into a useful end product. 

We divide the data engineering lifecycle into five stages:
- Generation
- Storage
- Ingestion
- Transformation
- Serving data


==***1. Generating Data?***==
-  Source systems are the origin of the data used in the data engineering lifecycle. For example, an application message queue, or a transactional database. The data engineer needs to have a working understanding of the way source systems work, the way they generate data, the frequency and velocity of the data, and the variety of data they generate. Engineers also need to keep an open line of communication with source system owners on changes that could break pipelines and analytics. 

- One of the most challenging nuances of source data is the ==schema==. The schema defines the hierarchical organization of data. Logically, we can think of data at the level of a whole source system, drilling down into individual tables, all the way to the structure of respective fields. The schema of data shipped from source systems is handled in various ways. Two popular options are schema-less and fixed schema. (Input from a webpage form, versus flat files, or plain text)

==***2. Storing Data?***==
-  You need a place to store data. Choosing a storage solution is key to success in the rest of the data lifecycle, and it’s also one of the most complicated stages of the data lifecycle for a variety of reasons. Storage runs across the entire data engineering lifecycle, often occurring in multiple places in a data pipeline, with storage systems crossing over with source systems, ingestion, transformation, and serving. In many ways, the way data is stored impacts how it is used in all of the stages of the data engineering lifecycle.

- Retrieval patterns will greatly vary based on the data being stored and queried. This brings up the notion of the ==temperatures of data==. Data that is most frequently accessed is called hot data. Hot data is commonly retrieved many times per day, perhaps even several times per second. This data should be stored for fast retrieval, where “fast” is relative to the use case. Lukewarm data might be accessed every so often—say, every week or month. Cold data is seldom queried and is appropriate for storing in an archival system. Cold data is often retained for compliance purposes or in case of a catastrophic failure in another system. 

==***3. Ingesting Data?***==
-  The next stage of the data engineering lifecycle is data ingestion from source systems. In our experience, source systems and ingestion represent the most significant bottlenecks of the data engineering lifecycle. The source systems are normally outside your direct control and might randomly become unresponsive or provide data of poor quality. As a result, data flow stops or delivers insufficient data for storage, processing, and serving. Unreliable source and ingestion systems have a ripple effect across the data engineering lifecycle. 

- ==Batch ingestion== is simply a specialized and convenient way of processing this stream in large chunks—for example, handling a full day’s worth of data in a single batch. ==Streaming ingestion== allows us to provide data to downstream systems—whether other applications, databases, or analytics systems—in a continuous, real-time fashion. Here, real-time (or near real-time) means that the data is available to a downstream system a short time after it is produced (e.g., less than one second later). The latency required to qualify as real-time varies by domain and requirements.

- In the ==push model== of data ingestion, a source system writes data out to a target, whether a database, object store, or filesystem. In the ==pull model==, data is retrieved from the source system. The line between the push and pull paradigms can be quite blurry

==***4. Transforming Data?***==
-  The next stage of the data engineering lifecycle is transformation, meaning data needs to be changed from its original form into something useful for downstream use cases. Without proper transformations, data will sit inert, and not be in a useful form for reports, analysis, or ML. Typically, the transformation stage is where data begins to create value for downstream user consumption.

==***5. Serving Data?***==
-  Now that the data has been ingested, stored, and transformed into coherent and useful structures, it’s time to get value from your data. “Getting value” from data means different things to different users. Data that is not consumed or queried is simply inert. The cloud era is triggering a new wave of vanity projects built on the latest data warehouses, object storage systems, and streaming technologies. Data projects must be intentional across the lifecycle. What is the ultimate business purpose of the data so carefully collected, cleaned, and stored?