*Notes Taken From: Spark The Definitive Guide* - Bill Chambers, Matei Zaharia


==***1. Spark Background? 

Apache Spark is a unified computing engine and a set of libraries for parallel data processing on computer clusters. Spark supports multiple widely used programming languages (Python, Java, Scala, and R).

Due to hard limits in heat dissipation, hardware developers stopped making individual processors faster, and switched toward adding more parallel CPU cores all running at the same speed. This change meant that suddenly applications needed to be modified to add parallelism in order to run faster, which set the stage for new programming models such as Apache Spark. 

The end result is a world in which collecting data is extremely inexpensive—many organizations
today even consider it negligent not to log data of possible relevance to the business—but
processing it requires large, parallel computations, often on clusters of machines. Moreover, in
this new world, the software developed in the past 50 years cannot automatically scale up, and
neither can the traditional programming models for data processing applications, creating the
need for new programming models. It is this world that Apache Spark was built for.


==***2. Dataframes / Partitions? 

A DataFrame is the most common Structured API and simply represents a table of data with
rows and columns. The list that defines the columns and the types within those columns is called
the schema. You can think of a DataFrame as a spreadsheet with named columns. The reason for putting the data on more than one computer should be intuitive: either the data is too large to fit on one machine or it would simply take too long to perform that computation on one machine.

To allow every executor to perform work in parallel, Spark breaks up the data into chunks called
partitions. A partition is a collection of rows that sit on one physical machine in your cluster. If you have one partition, Spark will have a parallelism of only one, even if you have thousands of executors. If you have many partitions but only one executor, Spark will still have a parallelism of only one because there is only one computation resource.


==***3. Transformations? 

In Spark, the core data structures are immutable, meaning they cannot be changed after they’re
created. This might seem like a strange concept at first: if you cannot change it, how are you
supposed to use it? To “change” a DataFrame, you need to instruct Spark how you would like to
modify it to do what you want. These instructions are called transformations. 

This is because we specified only an abstract transformation, and Spark will not act on transformations until we call an action. Transformations are the core of how you express your business logic using Spark. There are two types of transformations: those that specify narrow dependencies, and those that specify wide dependencies.

Transformations consisting of narrow dependencies (we’ll call them narrow transformations) are those for which each input partition will contribute to only one output partition. A wide dependency (or wide transformation) style transformation will have input partitions
contributing to many output partitions. You will often hear this referred to as a shuffle whereby
Spark will exchange partitions across the cluster. With narrow transformations, Spark will
automatically perform an operation called pipelining, meaning that if we specify multiple filters
on DataFrames, they’ll all be performed in-memory. The same cannot be said for shuffles. When we perform a shuffle, Spark writes the results to disk. 


==***4. Lazy Execution? 

Lazy evaluation means that Spark will wait until the very last moment to execute the graph of
computation instructions. In Spark, instead of modifying the data immediately when you express
some operation, you build up a plan of transformations that you would like to apply to your
source data. By waiting until the last minute to execute the code, Spark compiles this plan from
your raw DataFrame transformations to a streamlined physical plan that will run as efficiently as
possible across the cluster. This provides immense benefits because Spark can optimize the entire data flow from end to end. 

An example of this is something called predicate pushdown on DataFrames. If we build a large Spark job but specify a filter at the end that only requires us to fetch one row from our source data, the most efficient way to execute this is to access the single record that we need. Spark will actually optimize this for us by pushing the filter down automatically.


==***5. Putting An Example Together

```
# Import Data As Spark Dataframe
flightData2015 = spark.read.option("inferSchema", "true")\
	                       .option("header",      "true")\
	                       .csv("/data/flight-data/csv/2015-summary.csv")
	                       
# Create A Table/View
flightData2015.createOrReplaceTempView("flightdata2015")


# Do Some Transformation On That Table
sqlWay = spark.sql("""SELECT DEST_COUNTRY_NAME, count(1)
                      FROM flightdata2015
                      GROUP BY DEST_COUNTRY_NAME
                      """)

# Be Sure To Use Built In Functions As They Are Transformations Not Actions
from pyspark.sql.functions import max
flightData2015.select(max("count")).take(1)


```


==***6. Spark SQL Vs Dataframe API

-- Spark SQL --
**Pros**:
- Familiar to SQL users, leveraging skills from relational databases.
- Concise for complex queries like joins or aggregations [Spark SQL Inner Join vs. Outer Join](https://www.sparkcodehub.com/spark/sql/inner-vs-outer-join).
- Readable for non-programmers, such as data analysts.

**Cons**:
- String-based queries lack compile-time checks, risking runtime errors.
- Less flexible for dynamic logic or programmatic workflows.


-- Dataframe API --
**Pros**:
- Type-safe and compile-time checked, reducing errors.
- Flexible for dynamic logic, such as iterating over columns or conditional operations [Spark DataFrame Filter](https://www.sparkcodehub.com/spark/dataframe/filter).
- Seamless integration with Scala/Java/Python codebases.

**Cons**:
- Steeper learning curve for non-programmers.
- Verbose for complex SQL-like queries compared to SQL syntax.


**Verdict**: Use Spark SQL for analyst-friendly queries; use the DataFrame API for developer-driven, programmatic workflows. Both APIs leverage Spark SQL’s execution engine, ensuring identical performance for equivalent operations.


Additional Resource: 
+ https://soutir.substack.com/p/pyspark-vs-sql-syntax-breakdown


```
# Query In Python
sql = """ SELECT
			DEST_COUNTRY__NAME, 
			sum(count) as destination
		
		  FROM flightdata2015
		  GROUP BY DEST_COUNTRY__NAME
		  ORDER BY sum(count) DESC
		  LIMIT 5 """

maxSql = spark.sql(sql)
maxSql.show()


# Query In Spark
from pyspark.sql.functions import desc
flightData2015.groupBy("DEST_COUNTRY__NAME")\
              .sum("count")\
              .withColumnRenamed("sum(count)", "destinationtotal")\
              .sort(desc("destinationtotal"))\
              .limit(5)\
              .show()

```
